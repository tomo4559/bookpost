#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
bookpost - Book Review Auto Poster
ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆçµ±åˆç‰ˆï¼‰
"""

import os
import sys
import json
import logging
import argparse
import requests
import time
from bs4 import BeautifulSoup
from pathlib import Path

print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ èµ·å‹•...")  # ãƒ‡ãƒãƒƒã‚°ç”¨

def setup_logger(name="bookpost", log_file="data/logs/app.log"):
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    print(f"ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–: {log_file}")
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    log_dir = os.path.dirname(log_file)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        print(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {log_dir}")
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
    if logger.handlers:
        logger.handlers.clear()
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    return logger


def normalize_isbn(isbn):
    """ISBNã‚’13æ¡ãƒã‚¤ãƒ•ãƒ³ãªã—å½¢å¼ã«çµ±ä¸€"""
    isbn_clean = isbn.replace('-', '').replace(' ', '')
    
    if len(isbn_clean) == 13:
        return isbn_clean
    elif len(isbn_clean) == 10:
        isbn13 = '978' + isbn_clean[:-1]
        check_digit = calculate_isbn13_check_digit(isbn13)
        return isbn13 + str(check_digit)
    else:
        raise ValueError(f"ç„¡åŠ¹ãªISBNå½¢å¼: {isbn}")


def calculate_isbn13_check_digit(isbn12):
    """ISBN-13ã®ãƒã‚§ãƒƒã‚¯ãƒ‡ã‚£ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—"""
    total = 0
    for i, digit in enumerate(isbn12):
        weight = 1 if i % 2 == 0 else 3
        total += int(digit) * weight
    check_digit = (10 - (total % 10)) % 10
    return check_digit


def fetch_book_data(isbn, logger):
    """Google Books APIã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—"""
    isbn_normalized = normalize_isbn(isbn)
    logger.info(f"ISBNæ­£è¦åŒ–: {isbn} â†’ {isbn_normalized}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    cache_dir = "data/books"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {cache_dir}")
    
    cache_path = os.path.join(cache_dir, f"book_{isbn_normalized}.json")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    if os.path.exists(cache_path):
        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {cache_path}")
        with open(cache_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # Google Books APIå‘¼ã³å‡ºã—
    logger.info(f"Google Books APIã‹ã‚‰å–å¾—: ISBN={isbn_normalized}")
    api_url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn_normalized}"
    
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if 'items' not in data or len(data['items']) == 0:
            logger.error(f"è©²å½“ã™ã‚‹ISBNãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {isbn_normalized}")
            raise ValueError(f"è©²å½“ã™ã‚‹ISBNãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        volume_info = data['items'][0]['volumeInfo']
        book_data = {
            'isbn': isbn_normalized,
            'title': volume_info.get('title', ''),
            'authors': volume_info.get('authors', []),
            'publisher': volume_info.get('publisher', ''),
            'published_date': volume_info.get('publishedDate', ''),
            'description': volume_info.get('description', ''),
            'page_count': volume_info.get('pageCount', 0),
            'categories': volume_info.get('categories', []),
            'image_url': volume_info.get('imageLinks', {}).get('thumbnail', ''),
            'language': volume_info.get('language', 'ja')
        }
        
        logger.info(f"æ›¸ç±æƒ…å ±å–å¾—: {book_data['title']}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(book_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {cache_path}")
        return book_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"APIã‚¨ãƒ©ãƒ¼: {e}")
        raise Exception(f"Google Books APIé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")


def scrape_amazon_reviews(isbn, logger):
    """Amazonã‹ã‚‰ISBNæ¤œç´¢ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—"""
    logger.info(f"Amazonå•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—: ISBN={isbn}")
    
    # Amazonã®ISBNæ¤œç´¢URL
    amazon_url = f"https://www.amazon.co.jp/s?k={isbn}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
    }
    
    results = []
    
    try:
        # Amazonæ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        logger.info(f"Amazonæ¤œç´¢: {amazon_url}")
        response = requests.get(amazon_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # æœ€åˆã®å•†å“ãƒªãƒ³ã‚¯ã‚’å–å¾—
        product_link = soup.find('a', class_='a-link-normal s-no-outline')
        if product_link:
            product_url = 'https://www.amazon.co.jp' + product_link.get('href', '')
            logger.info(f"å•†å“ãƒšãƒ¼ã‚¸ç™ºè¦‹: {product_url}")
            
            # å•†å“ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            time.sleep(1)  # è² è·è»½æ¸›
            response = requests.get(product_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
            review_elements = soup.find_all('div', {'data-hook': 'review'}, limit=5)
            
            for i, review in enumerate(review_elements, 1):
                try:
                    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¿ã‚¤ãƒˆãƒ«
                    title_elem = review.find('a', {'data-hook': 'review-title'})
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    
                    # è©•ä¾¡
                    rating_elem = review.find('i', {'data-hook': 'review-star-rating'})
                    rating = rating_elem.get_text(strip=True) if rating_elem else ''
                    
                    # ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ¬æ–‡
                    body_elem = review.find('span', {'data-hook': 'review-body'})
                    body = body_elem.get_text(strip=True) if body_elem else ''
                    
                    if title or body:
                        results.append({
                            'number': i,
                            'source': 'Amazon',
                            'title': title,
                            'rating': rating,
                            'content': body,
                            'url': product_url
                        })
                        logger.info(f"  [Amazon-{i}] {title[:30]}...")
                        
                except Exception as e:
                    logger.warning(f"Amazonãƒ¬ãƒ“ãƒ¥ãƒ¼è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
    except Exception as e:
        logger.warning(f"Amazonå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return results


def scrape_google_search(search_term, logger):
    """Googleæ¤œç´¢ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ãƒˆæƒ…å ±ã‚’å–å¾—"""
    logger.info(f"Googleæ¤œç´¢: {search_term}")
    
    search_query = f"{search_term} æ›¸è©• ãƒ¬ãƒ“ãƒ¥ãƒ¼"
    # Googleæ¤œç´¢URLï¼ˆUser-Agentã‚’è¨­å®šã—ãªã„ã¨ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ï¼‰
    search_url = f"https://www.google.com/search?q={search_query}&hl=ja"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
    }
    
    results = []
    
    try:
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # Googleæ¤œç´¢çµæœï¼ˆé€šå¸¸ã®æ¤œç´¢çµæœï¼‰
        search_results = soup.find_all('div', class_='g', limit=10)
        
        logger.info(f"Googleæ¤œç´¢çµæœ: {len(search_results)}ä»¶")
        
        for i, result in enumerate(search_results, 1):
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯
                title_elem = result.find('h3')
                if not title_elem:
                    continue
                
                link_elem = result.find('a')
                if not link_elem:
                    continue
                
                url = link_elem.get('href', '')
                title = title_elem.get_text(strip=True)
                
                # ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
                snippet_elem = result.find('div', class_='VwiC3b')
                if not snippet_elem:
                    snippet_elem = result.find('span', class_='aCOpRe')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                
                results.append({
                    'number': i,
                    'source': 'Google',
                    'title': title,
                    'url': url,
                    'snippet': snippet
                })
                
                logger.info(f"  [Google-{i}] {title[:50]}...")
                
            except Exception as e:
                logger.warning(f"Googleæ¤œç´¢çµæœè§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
    except Exception as e:
        logger.warning(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    return results


def scrape_reviews(isbn, search_term, logger):
    """Googleæ¤œç´¢ + Amazonå•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›†ï¼ˆ2æ®µéšï¼‰"""
    logger.info(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›†é–‹å§‹ï¼ˆ2æ®µéšå–å¾—ï¼‰: {search_term}")
    
    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    review_dir = "data/reviews"
    if not os.path.exists(review_dir):
        os.makedirs(review_dir)
        logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {review_dir}")
    
    review_path = os.path.join(review_dir, f"review_{isbn}.txt")
    
    all_results = []
    
    # Phase 1: Googleæ¤œç´¢
    logger.info("Phase 1: Googleæ¤œç´¢ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ãƒˆæƒ…å ±å–å¾—")
    google_results = scrape_google_search(search_term, logger)
    all_results.extend(google_results)
    
    # Phase 2: Amazonç›´æ¥å–å¾—
    time.sleep(2)  # è² è·è»½æ¸›
    logger.info("Phase 2: Amazonå•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—")
    amazon_results = scrape_amazon_reviews(isbn, logger)
    all_results.extend(amazon_results)
    
    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    if len(all_results) == 0:
        logger.info("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        review_text = "â€» ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\n\n"
        review_text += f"æ›¸ç±: {search_term}\n"
        review_text += f"ISBN: {isbn}\n"
        review_text += f"åé›†æ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        review_text += "ã€å¯¾å‡¦æ–¹æ³•ã€‘\n"
        review_text += "1. Amazonç­‰ã§æ‰‹å‹•æ¤œç´¢ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼\n"
        review_text += "2. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ç›´æ¥è²¼ã‚Šä»˜ã‘ã¦ä¿å­˜\n"
        review_text += "3. ChatGPT/Perplexityã§è¨˜äº‹ç”Ÿæˆæ™‚ã«ä½¿ç”¨\n"
    else:
        review_text = f"æ›¸ç±ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„\n"
        review_text += "="*70 + "\n"
        review_text += f"æ›¸ç±: {search_term}\n"
        review_text += f"ISBN: {isbn}\n"
        review_text += f"åé›†æ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        review_text += f"åé›†ä»¶æ•°: {len(all_results)}ä»¶ï¼ˆGoogle: {len(google_results)}ä»¶, Amazon: {len(amazon_results)}ä»¶ï¼‰\n"
        review_text += "="*70 + "\n\n"
        
        for result in all_results:
            if result['source'] == 'Google':
                review_text += f"ã€{result['number']}ã€‘ {result['title']}\n"
                review_text += f"å‡ºå…¸: Googleæ¤œç´¢çµæœ\n"
                review_text += f"URL: {result['url']}\n"
                review_text += f"è¦ç´„: {result['snippet']}\n"
            else:  # Amazon
                review_text += f"ã€{result['number']}ã€‘ {result['title']}\n"
                review_text += f"å‡ºå…¸: Amazon ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
                review_text += f"è©•ä¾¡: {result['rating']}\n"
                review_text += f"å†…å®¹: {result['content'][:200]}...\n"
                review_text += f"URL: {result['url']}\n"
            
            review_text += "-"*70 + "\n\n"
        
        review_text += "="*70 + "\n"
        review_text += "â€» ä¸Šè¨˜ã¯Googleæ¤œç´¢çµæœã¨Amazonãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è¦ç´„ã§ã™\n"
        review_text += "â€» ChatGPT/Perplexityã§è¨˜äº‹ç”Ÿæˆæ™‚ã«å‚è€ƒã«ã—ã¦ãã ã•ã„\n"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(review_path, 'w', encoding='utf-8') as f:
        f.write(review_text)
    
    logger.info(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¿å­˜: {review_path} ({len(all_results)}ä»¶)")
    return review_path


def cmd_fetch(args, logger):
    """fetchã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
    isbn = args.isbn
    logger.info(f"=== fetché–‹å§‹: ISBN={isbn} ===")
    print(f"\nğŸ“š æ›¸ç±æƒ…å ±å–å¾—é–‹å§‹: ISBN={isbn}")
    
    try:
        # æ›¸ç±æƒ…å ±å–å¾—
        print("1. Google Books APIã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—ä¸­...")
        book_data = fetch_book_data(isbn, logger)
        isbn_normalized = book_data['isbn']
        print(f"   âœ… å–å¾—å®Œäº†: {book_data['title']}")
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›†ï¼ˆæ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ï¼‰
        print("2. Bingæ¤œç´¢ã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’åé›†ä¸­...")
        search_term = f"{book_data['title']} {' '.join(book_data.get('authors', []))}"
        review_path = scrape_reviews(isbn_normalized, search_term, logger)
        print(f"   âœ… åé›†å®Œäº†: {review_path}")
        
        logger.info("=== fetchå®Œäº† ===")
        
        print("\n" + "="*60)
        print("âœ… æ›¸ç±æƒ…å ±ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("="*60)
        print(f"ğŸ“– æ›¸ç±å: {book_data['title']}")
        print(f"âœï¸  è‘—è€…: {', '.join(book_data['authors'])}")
        print(f"ğŸ“… å‡ºç‰ˆæ—¥: {book_data['published_date']}")
        print(f"ğŸ“„ ãƒ¬ãƒ“ãƒ¥ãƒ¼: {review_path}")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. ChatGPT/Perplexityã§è¨˜äº‹ã‚’ç”Ÿæˆ")
        print(f"   å…¥åŠ›: {review_path}")
        print(f"   å‡ºåŠ›: data/outputs/article_{isbn_normalized}.md")
        print("2. ç”»åƒã‚’ç”Ÿæˆ")
        print(f"   å‡ºåŠ›: data/images/thumbnail_{isbn_normalized}.png")
        
    except Exception as e:
        logger.error(f"fetchã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


def cmd_post(args, logger):
    """postã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œï¼ˆè©¦ä½œç‰ˆï¼‰"""
    isbn = normalize_isbn(args.isbn)
    logger.info(f"=== posté–‹å§‹: ISBN={isbn} ===")
    print(f"\nğŸ“ æŠ•ç¨¿æº–å‚™ç¢ºèª: ISBN={isbn}")
    
    try:
        book_path = f"data/books/book_{isbn}.json"
        article_path = f"data/outputs/article_{isbn}.md"
        image_path = f"data/images/thumbnail_{isbn}.png"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        print("ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªä¸­...")
        
        if not os.path.exists(book_path):
            raise FileNotFoundError(f"æ›¸ç±æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {book_path}")
        
        with open(book_path, 'r', encoding='utf-8') as f:
            book_data = json.load(f)
        print(f"  âœ… æ›¸ç±æƒ…å ±: {book_path}")
        
        if not os.path.exists(article_path):
            raise FileNotFoundError(f"è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {article_path}")
        
        with open(article_path, 'r', encoding='utf-8') as f:
            article_content = f.read()
        print(f"  âœ… è¨˜äº‹: {article_path} ({len(article_content)}æ–‡å­—)")
        
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        
        image_size_mb = os.path.getsize(image_path) / (1024 * 1024)
        print(f"  âœ… ç”»åƒ: {image_path} ({image_size_mb:.2f}MB)")
        
        if image_size_mb > 2:
            print(f"  âš ï¸  ç”»åƒãŒ2MBè¶…é: {image_size_mb:.2f}MB")
        
        print("\n" + "="*60)
        print("âœ… æŠ•ç¨¿æº–å‚™ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªå®Œäº†")
        print("="*60)
        print(f"ğŸ“– æ›¸ç±å: {book_data['title']}")
        print("ã€è©¦ä½œç‰ˆã€‘WordPressæŠ•ç¨¿æ©Ÿèƒ½ã¯æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã§å®Ÿè£…")
        
        logger.info("=== postç¢ºèªå®Œäº† ===")
        
    except FileNotFoundError as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"postã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("main()é–¢æ•°é–‹å§‹")  # ãƒ‡ãƒãƒƒã‚°
    
    # ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
    logger = setup_logger()
    logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ èµ·å‹•")
    
    # å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼
    parser = argparse.ArgumentParser(description='bookpost - Book Review Auto Poster')
    subparsers = parser.add_subparsers(dest='command', help='ã‚³ãƒãƒ³ãƒ‰')
    
    # fetchã‚³ãƒãƒ³ãƒ‰
    parser_fetch = subparsers.add_parser('fetch', help='æ›¸ç±æƒ…å ±å–å¾—')
    parser_fetch.add_argument('--isbn', required=True, help='ISBN-13')
    
    # postã‚³ãƒãƒ³ãƒ‰
    parser_post = subparsers.add_parser('post', help='æŠ•ç¨¿æº–å‚™ç¢ºèª')
    parser_post.add_argument('--isbn', required=True, help='ISBN-13')
    
    # å¼•æ•°è§£æ
    args = parser.parse_args()
    print(f"ã‚³ãƒãƒ³ãƒ‰: {args.command}")  # ãƒ‡ãƒãƒƒã‚°
    
    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == 'fetch':
        cmd_fetch(args, logger)
    elif args.command == 'post':
        cmd_post(args, logger)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œé–‹å§‹")  # ãƒ‡ãƒãƒƒã‚°
    main()
    print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œçµ‚äº†")  # ãƒ‡ãƒãƒƒã‚°